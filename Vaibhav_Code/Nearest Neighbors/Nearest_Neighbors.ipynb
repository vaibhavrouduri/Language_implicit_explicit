{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nearest_Neighbors.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOW6Q8qp1W/9Z9Zi4kqf7hP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaibhavrouduri/Thesis-Work/blob/main/Vaibhav_Code/Nearest%20Neighbors/Nearest_Neighbors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfAbJSmGM4Pq"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "from itertools import filterfalse\n",
        "from scipy import spatial\n",
        "%matplotlib notebook\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "import gensim\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7VYtz9LNwSC"
      },
      "source": [
        "# Glove Common Crawl"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEmweUUiNDb3"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.840B.300d.zip # Retrieves content from the given URL\n",
        "!unzip -q glove.840B.300d.zip # Unzips the file and returns the .txt file which can be used for analysis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Oir_LhjNMHE"
      },
      "source": [
        "embeddings_glove = {} # Empty dictionary in which the words in the vocabulary and the corresponding vector will be added\n",
        "with open(\"glove.840B.300d.txt\") as f:\n",
        "    for line in f: # Each line in the file contains the token followed by the vector representation of that token trained in glove in 300 dimensions separated by a space. Each dimension of the vector is separated by a space as well\n",
        "        word, coefs = line.split(maxsplit=1) # Splits the line as described above at the first space, hence the token gets separated from the vector. The token is stored in 'word' and the vector is stored in 'coefs'\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \") # Returns the vector representation in the form of an array which can now be used for analysis (cosine similarity, etc)\n",
        "        embeddings_glove[word] = coefs # Add the token and its corresponding vector into the dictionary as key and value pair. Now we can call the vector of any token in the vocabulary via the dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cqg0IBsN1x6"
      },
      "source": [
        "# Glove Wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9S2nwLJNN8l",
        "outputId": "be67dd2e-70a0-4691-f117-54a34f54dc02"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip # Retrieves content from the given URL\n",
        "!unzip -q glove.6B.zip # Unzips the file and returns the .txt file which can be used for analysis. In this case, returns 4 files, having 50, 100, 200, and 300 dimensional vectors. We are choosing to use the 300 dimensional vectors"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-29 19:46:57--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-11-29 19:46:57--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-11-29 19:46:57--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  4.99MB/s    in 2m 40s  \n",
            "\n",
            "2021-11-29 19:49:37 (5.14 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI2lZp0ZNQ_r"
      },
      "source": [
        "embeddings_glove_wiki_gw = {} # Empty dictionary in which the words in the vocabulary and the corresponding vector will be added\n",
        "with open(\"glove.6B.300d.txt\") as f:\n",
        "    for line in f: # Each line in the file contains the token followed by a the vector representation of that token trained in glove in 300 dimensions separated by a space. Each dimension of the vector is separated by a space as well\n",
        "        word, coefs = line.split(maxsplit=1) # Splits the line as described above at the first space, hence the token gets separated from the vector. The token is stored in 'word' and the vector is stored in 'coefs'\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \") # Returns the vector representation in the form of an array which can now be used for analysis (cosine similarity, etc)\n",
        "        embeddings_glove_wiki_gw[word] = coefs # Add the token and its corresponding vector into the dictionary as key and value pair. Now we can call the vector of any token in the vocabulary via the dictionary"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwTcESBTN5vM"
      },
      "source": [
        "# Fastext Common Crawl"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MdHCgRXNVPG"
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip # Retrieves content from the given URL\n",
        "!unzip -q crawl-300d-2M.vec.zip # Unzips the file and returns the .vec file which can be used for analysis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrHpEmV4NW65"
      },
      "source": [
        "embeddings_fastext_cc = {}\n",
        "with open(\"crawl-300d-2M.vec\") as f:\n",
        "    for line in f: # Each line in the file contains the token followed by a the vector representation of that token trained in glove in 300 dimensions separated by a space. Each dimension of the vector is separated by a space as well\n",
        "        word, coefs = line.split(maxsplit=1) # Splits the line as described above at the first space, hence the token gets separated from the vector. The token is stored in 'word' and the vector is stored in 'coefs'\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \") # Returns the vector representation in the form of an array which can now be used for analysis (cosine similarity, etc)\n",
        "        embeddings_fastext_cc[word] = coefs # Add the token and its corresponding vector into the dictionary as key and value pair. Now we can call the vector of any token in the vocabulary via the dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRtXfmxmN-p7"
      },
      "source": [
        "# Fastext Wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIgW2BeRNZzh"
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip # Retrieves content from the given URL\n",
        "!unzip -q wiki-news-300d-1M.vec.zip # Unzips the file and returns the .vec file which can be used for analysis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12Wi6fClNbq9"
      },
      "source": [
        "embeddings_fastext_wiki = {}\n",
        "with open(\"wiki-news-300d-1M.vec\") as f:\n",
        "    for line in f: # Each line in the file contains the token followed by a the vector representation of that token trained in glove in 300 dimensions separated by a space. Each dimension of the vector is separated by a space as well\n",
        "        word, coefs = line.split(maxsplit=1) # Splits the line as described above at the first space, hence the token gets separated from the vector. The token is stored in 'word' and the vector is stored in 'coefs'\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \") # Returns the vector representation in the form of an array which can now be used for analysis (cosine similarity, etc)\n",
        "        embeddings_fastext_cc[word] = coefs # Add the token and its corresponding vector into the dictionary as key and value pair. Now we can call the vector of any token in the vocabulary via the dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZNWc4V4OF0S"
      },
      "source": [
        "# word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4nu550LOtJu"
      },
      "source": [
        "!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_CdG2zLNrqR"
      },
      "source": [
        "EMBEDDING_FILE = '/root/input/GoogleNews-vectors-negative300.bin.gz'\n",
        "embeddings_word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkWGv-e0O3OO"
      },
      "source": [
        "# Nearest Neighbors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibfTTO_sRb1i"
      },
      "source": [
        "def cos(v1, v2): \n",
        "  \"\"\"Returns the cosine similarity of 2 vectors of same dimesionality.\n",
        "\n",
        "    Arguments:\n",
        "    v1 -- Vector 1\n",
        "    v2 -- Vector 2\n",
        "    \"\"\"\n",
        "\n",
        "  num = np.dot(v1, v2)\n",
        "  denom = np.linalg.norm(v1) * np.linalg.norm(v2)\n",
        "  return num/denom"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mgnqu5acO5id"
      },
      "source": [
        "def vocab_to_list(embedding_type):\n",
        "    \"\"\"Returns the list of words in the vocabulary of a chosen embedding type(eg. glove with wikipedia)\n",
        "\n",
        "    Arguments:\n",
        "    embedding_type -- The dictionary for the corpus algorithm pair being used in the current analysis (eg. for glove with common crawl, use embeddings_glove)\n",
        "    \"\"\"\n",
        "  words = []\n",
        "  for w in embedding_type:\n",
        "    words.append(w)\n",
        "  return words"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL1CSlCvQTLk"
      },
      "source": [
        "def similar_words(word, number, embedding_type):\n",
        "  \"\"\"Returns a chosen number of most similar words to a given word in terms of cosine similarity.\n",
        "\n",
        "    Arguments:\n",
        "    word -- The token whose most similar words you want to find\n",
        "    number -- The number of similar words you want to find\n",
        "    \"\"\"\n",
        "  similar_word = []\n",
        "\n",
        "  words_for_word = vocab_to_list(embedding_type).copy()\n",
        "\n",
        "  for w in words_for_word:\n",
        "    if embedding_type[word].shape != embedding_type[w].shape:\n",
        "      words_for_word.remove(w)\n",
        "\n",
        "  for w in words_for_word: # For some reason unless I run this code snippet twice all the words with different vector shapes aren't getting removed.\n",
        "    if embedding_type[word].shape != embedding_type[w].shape:\n",
        "      words_for_word.remove(w)\n",
        "\n",
        "  for w in words_for_word:\n",
        "    similar_word.append(cos(embedding_type[word], embedding_type[w]))\n",
        "  \n",
        "  similar_word_copy = similar_word.copy()\n",
        "\n",
        "  similar_word.sort(reverse = True)\n",
        "\n",
        "  relevant_indices = []\n",
        "\n",
        "  for i in range(number + 1):\n",
        "    relevant_indices.append(similar_word_copy.index(similar_word[i]))\n",
        "\n",
        "  ans = []\n",
        "\n",
        "  for i in relevant_indices:\n",
        "    ans.append(words_for_word[i])\n",
        "  \n",
        "  ans.remove(word)\n",
        "\n",
        "  return ans"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWmgLwPpSIBs"
      },
      "source": [
        "# Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsC4Yw5FRFyK",
        "outputId": "b8f7ea0d-cca7-4f23-eb4f-6424363b8e85"
      },
      "source": [
        "similar_words(\"skeptic\", 10, embeddings_glove_wiki_gw)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['skeptics',\n",
              " 'sceptic',\n",
              " 'sceptics',\n",
              " 'believer',\n",
              " 'environmentalist',\n",
              " 'proponent',\n",
              " 'freethinker',\n",
              " 'atheist',\n",
              " 'skeptical',\n",
              " 'ardent']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}