{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WEAT.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPMiX0M8272TvIdKp1tgCdH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaibhavrouduri/Thesis-Work/blob/main/Vaibhav_Code/WEAT/WEAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOPC9_Zy3OJA"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "from itertools import filterfalse\n",
        "from scipy import spatial\n",
        "%matplotlib notebook\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "import gensim\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyBZ1_gi3kLl"
      },
      "source": [
        "# Glove Common Crawl"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOePp3eN3amq"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.840B.300d.zip # Retrieves content from the given URL\n",
        "!unzip -q glove.840B.300d.zip # Unzips the file and returns the .txt file which can be used for analysis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QG3QGpZc3da9"
      },
      "source": [
        "embeddings_glove = {} # Empty dictionary in which the words in the vocabulary and the corresponding vector will be added\n",
        "with open(\"glove.840B.300d.txt\") as f:\n",
        "    for line in f: # Each line in the file contains the token followed by the vector representation of that token trained in glove in 300 dimensions separated by a space. Each dimension of the vector is separated by a space as well\n",
        "        word, coefs = line.split(maxsplit=1) # Splits the line as described above at the first space, hence the token gets separated from the vector. The token is stored in 'word' and the vector is stored in 'coefs'\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \") # Returns the vector representation in the form of an array which can now be used for analysis (cosine similarity, etc)\n",
        "        embeddings_glove[word] = coefs # Add the token and its corresponding vector into the dictionary as key and value pair. Now we can call the vector of any token in the vocabulary via the dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmEDyytj3q0l"
      },
      "source": [
        "# Glove Wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wi2cLhHn3gTC",
        "outputId": "8e4a4eaf-000a-4be9-a2f3-65298862b17a"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip # Retrieves content from the given URL\n",
        "!unzip -q glove.6B.zip # Unzips the file and returns the .txt file which can be used for analysis. In this case, returns 4 files, having 50, 100, 200, and 300 dimensional vectors. We are choosing to use the 300 dimensional vectors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-22 18:34:47--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-11-22 18:34:48--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-11-22 18:34:48--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.08MB/s    in 2m 40s  \n",
            "\n",
            "2021-11-22 18:37:28 (5.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjGyq8xe3zg6"
      },
      "source": [
        "embeddings_glove_wiki_gw = {} # Empty dictionary in which the words in the vocabulary and the corresponding vector will be added\n",
        "with open(\"glove.6B.300d.txt\") as f:\n",
        "    for line in f: # Each line in the file contains the token followed by a the vector representation of that token trained in glove in 300 dimensions separated by a space. Each dimension of the vector is separated by a space as well\n",
        "        word, coefs = line.split(maxsplit=1) # Splits the line as described above at the first space, hence the token gets separated from the vector. The token is stored in 'word' and the vector is stored in 'coefs'\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \") # Returns the vector representation in the form of an array which can now be used for analysis (cosine similarity, etc)\n",
        "        embeddings_glove_wiki_gw[word] = coefs # Add the token and its corresponding vector into the dictionary as key and value pair. Now we can call the vector of any token in the vocabulary via the dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1S69x1T32OU"
      },
      "source": [
        "# Fastext Common Crawl"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8rnDs3D34GA"
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip # Retrieves content from the given URL\n",
        "!unzip -q crawl-300d-2M.vec.zip # Unzips the file and returns the .vec file which can be used for analysis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDa3cukNYUg3"
      },
      "source": [
        "embeddings_fastext_cc = {}\n",
        "with open(\"crawl-300d-2M.vec\") as f:\n",
        "    for line in f: # Each line in the file contains the token followed by a the vector representation of that token trained in glove in 300 dimensions separated by a space. Each dimension of the vector is separated by a space as well\n",
        "        word, coefs = line.split(maxsplit=1) # Splits the line as described above at the first space, hence the token gets separated from the vector. The token is stored in 'word' and the vector is stored in 'coefs'\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \") # Returns the vector representation in the form of an array which can now be used for analysis (cosine similarity, etc)\n",
        "        embeddings_fastext_cc[word] = coefs # Add the token and its corresponding vector into the dictionary as key and value pair. Now we can call the vector of any token in the vocabulary via the dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeOiLSim4G0Y"
      },
      "source": [
        "# Fastext Wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvYtT6iq4GVi"
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip # Retrieves content from the given URL\n",
        "!unzip -q wiki-news-300d-1M.vec.zip # Unzips the file and returns the .vec file which can be used for analysis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZeZAG1T4QaT"
      },
      "source": [
        "embeddings_fastext_wiki = {}\n",
        "with open(\"wiki-news-300d-1M.vec\") as f:\n",
        "    for line in f: # Each line in the file contains the token followed by a the vector representation of that token trained in glove in 300 dimensions separated by a space. Each dimension of the vector is separated by a space as well\n",
        "        word, coefs = line.split(maxsplit=1) # Splits the line as described above at the first space, hence the token gets separated from the vector. The token is stored in 'word' and the vector is stored in 'coefs'\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \") # Returns the vector representation in the form of an array which can now be used for analysis (cosine similarity, etc)\n",
        "        embeddings_fastext_cc[word] = coefs # Add the token and its corresponding vector into the dictionary as key and value pair. Now we can call the vector of any token in the vocabulary via the dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDkYGNUD4jYm"
      },
      "source": [
        "# word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoU31LOs4kh4"
      },
      "source": [
        "!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3lGaXgo4mca"
      },
      "source": [
        "EMBEDDING_FILE = '/root/input/GoogleNews-vectors-negative300.bin.gz'\n",
        "embeddings_word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55Ibj09L49sS"
      },
      "source": [
        "# WEAT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVtK1PBK4-17"
      },
      "source": [
        "def cos(v1, v2): \n",
        "  \"\"\"Returns the cosine similarity of 2 vectors of same dimesionality.\n",
        "\n",
        "    Arguments:\n",
        "    v1 -- Vector 1\n",
        "    v2 -- Vector 2\n",
        "    \"\"\"\n",
        "\n",
        "  num = np.dot(v1, v2)\n",
        "  denom = np.linalg.norm(v1) * np.linalg.norm(v2)\n",
        "  return num/denom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfj81Bwx5Bvw"
      },
      "source": [
        "def weat_swAB(w, A, B, embedding_type):  \n",
        "  \"\"\"Returns the association of a single word with the chosen attribute (eg. good/bad).\n",
        "\n",
        "    Arguments:\n",
        "    w -- The string(word) whose association with the chosen attribute you want to find\n",
        "    A -- List of Attribute words representing 1 end of the spectrum (eg. for the attribute Good/Bad, the list of words representing 'Good')\n",
        "    B -- List of Attribute words representing the other end of the spectrum (eg. for the attribute Good/Bad, the list of words representing 'Bad')\n",
        "    embedding_type -- The dictionary for the corpus algorithm pair being used in the current analysis (eg. for glove with common crawl, use embeddings_glove)\n",
        "    \"\"\"\n",
        "  A_list = []\n",
        "  B_list = []\n",
        "\n",
        "  for v in A:\n",
        "    A_list.append(cos(embedding_type[w], embedding_type[v]))\n",
        "  for v in B:\n",
        "    B_list.append(cos(embedding_type[w], embedding_type[v]))\n",
        "\n",
        "  A_array = np.array(A_list)\n",
        "  B_array = np.array(B_list)\n",
        "\n",
        "  return np.mean(A_array) - np.mean(B_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmVYv8zk5D_G"
      },
      "source": [
        "def weat_ES(X, Y, A, B, embedding_type):\n",
        "  \"\"\"Returns the effect size of a pair of categories (eg Simple vs Difficult) with the chosen attribute (eg. good/bad).\n",
        "\n",
        "    Arguments:\n",
        "    X -- List of Category words representing 1 of the categories (eg. for Simple/Difficult, the list of words representing 'Simple')\n",
        "    Y -- List of Category words representing the other category (eg. for Simple/Difficult, the list of words representing 'Difficult')\n",
        "    A -- List of Attribute words representing 1 end of the spectrum (eg. for the attribute Good/Bad, the list of words representing 'Good')\n",
        "    B -- List of Attribute words representing the other end of the spectrum (eg. for the attribute Good/Bad, the list of words representing 'Bad')\n",
        "    embedding_type -- The dictionary for the corpus algorithm pair being used in the current analysis (eg. for glove with common crawl, use embeddings_glove)\n",
        "    \"\"\"\n",
        "  x_list = []\n",
        "  y_list = []\n",
        "\n",
        "  for x in X:\n",
        "    x_list.append(weat_swAB(x, A, B, embedding_type))\n",
        "  for y in Y:\n",
        "    y_list.append(weat_swAB(y, A, B, embedding_type))\n",
        "\n",
        "  x_array = np.array(x_list)\n",
        "  y_array = np.array(y_list)\n",
        "\n",
        "  return (np.mean(x_array) - np.mean(y_array))/(np.std(np.concatenate((x_array, y_array)), ddof = 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKN6ITBL5F13"
      },
      "source": [
        "def test_statistic(X, Y, A, B, embedding_type):\n",
        "  \"\"\"Returns the test statistic given the 2 sets of category stimuli and attribute stimuli.\n",
        "\n",
        "    Arguments:\n",
        "    X -- List of Category words representing 1 of the categories (eg. for Simple/Difficult, the list of words representing 'Simple')\n",
        "    Y -- List of Category words representing the other category (eg. for Simple/Difficult, the list of words representing 'Difficult')\n",
        "    A -- List of Attribute words representing 1 end of the spectrum (eg. for the attribute Good/Bad, the list of words representing 'Good')\n",
        "    B -- List of Attribute words representing the other end of the spectrum (eg. for the attribute Good/Bad, the list of words representing 'Bad')\n",
        "    embedding_type -- The dictionary for the corpus algorithm pair being used in the current analysis (eg. for glove with common crawl, use embeddings_glove)\n",
        "    \"\"\"\n",
        "  x_list = []\n",
        "  y_list = []\n",
        "\n",
        "  for x in X:\n",
        "    x_list.append(weat_swAB(x, A, B, embedding_type))\n",
        "  for y in Y:\n",
        "    y_list.append(weat_swAB(y, A, B, embedding_type))\n",
        "\n",
        "  x_array = np.array(x_list)\n",
        "  y_array = np.array(y_list)\n",
        "\n",
        "  return np.sum(x_array) - np.sum(y_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivmu2Oe45InP"
      },
      "source": [
        "def random_permutation(targets, r):\n",
        "  \"\"\"Returns a random permutation of a fixed number of a chosed list.\n",
        "\n",
        "    Arguments:\n",
        "    targets -- The list whose permutation is to be generated\n",
        "    r -- Size of the permutation\n",
        "    \"\"\"\n",
        "  return random.sample(targets, r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaZB4T1k5LQY"
      },
      "source": [
        "def p_value_weat(X, Y, A, B, embedding_type, iterations):\n",
        "  \"\"\"Returns the p-value given a pair of categories (eg Simple vs Difficult) and the chosen attribute (eg. good/bad).\n",
        "\n",
        "    Arguments:\n",
        "    X -- List of Category words representing 1 of the categories (eg. for Simple/Difficult, the list of words representing 'Simple')\n",
        "    Y -- List of Category words representing the other category (eg. for Simple/Difficult, the list of words representing 'Difficult')\n",
        "    A -- List of Attribute words representing 1 end of the spectrum (eg. for the attribute Good/Bad, the list of words representing 'Good')\n",
        "    B -- List of Attribute words representing the other end of the spectrum (eg. for the attribute Good/Bad, the list of words representing 'Bad')\n",
        "    embedding_type -- The dictionary for the corpus algorithm pair being used in the current analysis (eg. for glove with common crawl, use embeddings_glove)\n",
        "    iterations -- Number of permutations you want to generate (The more you generate, the more accurate the result is)\n",
        "    \"\"\"\n",
        "  X_Y = X + Y\n",
        "  size_of_permutation = len(X)\n",
        "  permutations = []\n",
        "  test_statistics_permutations = []\n",
        "  condition_satisfied = []\n",
        "  count = 0\n",
        "\n",
        "  while iterations != 0:\n",
        "    permutations.append(random_permutation(X_Y, size_of_permutation))\n",
        "    iterations = iterations - 1\n",
        "\n",
        "  for Xi in permutations:\n",
        "    count = count + 1\n",
        "    Yi = filterfalse(lambda w:w in Xi, X_Y)\n",
        "    test_statistics_permutations.append(test_statistic(Xi, Yi, A, B, embedding_type))\n",
        "\n",
        "  overall_test_statistic = test_statistic(X, Y, A, B, embedding_type)\n",
        "\n",
        "  condition_satisfied = [p > overall_test_statistic for p in test_statistics_permutations]\n",
        "\n",
        "  condition_satisfied_array = np.array(condition_satisfied)\n",
        "\n",
        "  return condition_satisfied_array.sum()/condition_satisfied_array.size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErEwPFwJBhgi"
      },
      "source": [
        "# Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6sp0HyvBZnH"
      },
      "source": [
        "# Attribute Stimuli\n",
        "Good = [\"love\", \"cheer\", \"friend\", \"pleasure\", \"paradise\", \"splendid\"]\n",
        "Bad = [\"abuse\", \"grief\", \"poison\", \"sadness\", \"pain\", \"bomb\"]\n",
        "\n",
        "# Category Stimuli\n",
        "Simple = [\"easy\", \"elementary\", \"straightforward\", \"effortless\"]\n",
        "Difficult = [\"complicated\", \"challenging\", \"puzzling\", \"baffling\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wfL947IB4Oj",
        "outputId": "e30220bc-4d85-48c3-fe71-2abf0e36eee8"
      },
      "source": [
        "weat_ES(Simple, Difficult, Good, Bad, embeddings_glove_wiki_gw)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.5131288"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGk73WGhCtVa",
        "outputId": "599d1a67-3591-49fc-c94f-2bf6cc35a76e"
      },
      "source": [
        "p_value_weat(Simple, Difficult, Good, Bad, embeddings_glove_wiki_gw, 1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH59NYKAC2Qt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}